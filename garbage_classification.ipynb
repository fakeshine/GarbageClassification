{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.signal as signalpy\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import keras_tuner as kt\n",
    "import kagglehub\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_folder(folder_path, image_extensions=(\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "    count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(image_extensions):  # Check for valid image extensions\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_jpg_images_in_folder(folder_path, image_extensions=(\".jpg\")):\n",
    "    count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(image_extensions):  # Check for valid image extensions\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def load_image_and_label(image_path , label) :\n",
    "    image_string = tf.io.read_file(image_path)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image_decoded.set_shape([224, 224, 3])   \n",
    "    return image_decoded, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the datasets and move them to the repo folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# kaggle_default_dataset_1_path = kagglehub.dataset_download(\"farzadnekouei/trash-type-image-dataset\")\n",
    "# print(\"Path to dataset 1 files :\", kaggle_default_dataset_1_path)\n",
    "# kaggle_default_dataset_2_path = kagglehub.dataset_download(\"mostafaabla/garbage-classification\")\n",
    "# print(\"Path to dataset 2 files :\", kaggle_default_dataset_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# kaggle_default_dataset_1_waste_categories_path = os.path.join(kaggle_default_dataset_1_path, 'TrashType_Image_Dataset')\n",
    "# kaggle_default_dataset_2_waste_categories_path = os.path.join(kaggle_default_dataset_2_path, 'garbage_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_folder_path = os.path.join(os.getcwd(),'Dataset')\n",
    "dataset_1_path = os.path.join(dataset_parent_folder_path, 'Dataset 1')\n",
    "dataset_2_path = os.path.join(dataset_parent_folder_path, 'Dataset 2')\n",
    "os.makedirs(dataset_parent_folder_path, exist_ok=True)\n",
    "os.makedirs(dataset_1_path, exist_ok=True)\n",
    "os.makedirs(dataset_2_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# shutil.copytree(kaggle_default_dataset_1_waste_categories_path, dataset_1_path, dirs_exist_ok=True)\n",
    "# shutil.rmtree(kaggle_default_dataset_1_path)\n",
    "# shutil.copytree(kaggle_default_dataset_2_waste_categories_path, dataset_2_path, dirs_exist_ok=True)\n",
    "# shutil.rmtree(kaggle_default_dataset_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give a generic structure to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waste Types Folders :\n",
    "1. general_waste\n",
    "2. paper_waste\n",
    "3. plastic_waste\n",
    "4. metal_waste\n",
    "5. textile_waste\n",
    "6. electronic_waste\n",
    "7. glass_waste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 1 modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'glass')) and not os.path.exists(os.path.join(dataset_1_path, 'glass_waste')) :\n",
    "#     os.replace(os.path.join(dataset_1_path, 'glass'), os.path.join(dataset_1_path, 'glass_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'metal')) and not os.path.exists(os.path.join(dataset_1_path, 'metal_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'metal'), os.path.join(dataset_1_path, 'metal_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'paper')) and not os.path.exists(os.path.join(dataset_1_path, 'paper_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'paper'), os.path.join(dataset_1_path, 'paper_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'plastic')) and not os.path.exists(os.path.join(dataset_1_path, 'plastic_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'plastic'), os.path.join(dataset_1_path, 'plastic_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'trash')) and not os.path.exists(os.path.join(dataset_1_path, 'general_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'trash'), os.path.join(dataset_1_path, 'general_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'cardboard')) :\n",
    "#     shutil.copytree(os.path.join(dataset_1_path, 'cardboard'), os.path.join(dataset_1_path, 'paper_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_1_path, 'cardboard'))\n",
    "\n",
    "dataset_1_general_waste_path = os.path.join(dataset_1_path, 'general_waste')\n",
    "dataset_1_glass_waste_path = os.path.join(dataset_1_path, 'glass_waste')\n",
    "dataset_1_metal_waste_path = os.path.join(dataset_1_path, 'metal_waste')\n",
    "dataset_1_paper_waste_path = os.path.join(dataset_1_path, 'paper_waste')\n",
    "dataset_1_plastic_waste_path = os.path.join(dataset_1_path, 'plastic_waste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2 modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'battery')) and not os.path.exists(os.path.join(dataset_2_path, 'electronic_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'battery'), os.path.join(dataset_2_path, 'electronic_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'trash')) and not os.path.exists(os.path.join(dataset_2_path, 'general_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'trash'), os.path.join(dataset_2_path, 'general_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'metal')) and not os.path.exists(os.path.join(dataset_2_path, 'metal_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'metal'), os.path.join(dataset_2_path, 'metal_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'paper')) and not os.path.exists(os.path.join(dataset_2_path, 'paper_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'paper'), os.path.join(dataset_2_path, 'paper_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'plastic')) and not os.path.exists(os.path.join(dataset_2_path, 'plastic_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'plastic'), os.path.join(dataset_2_path, 'plastic_waste'))\n",
    "\n",
    "# os.makedirs(os.path.join(dataset_2_path, 'glass_waste'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(dataset_2_path, 'textile_waste'), exist_ok=True)\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'biological')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'biological'), os.path.join(dataset_2_path, 'general_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'biological'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'brown-glass')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'brown-glass'), os.path.join(dataset_2_path, 'glass_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'brown-glass'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'green-glass')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'green-glass'), os.path.join(dataset_2_path, 'glass_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'green-glass'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'white-glass')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'white-glass'), os.path.join(dataset_2_path, 'glass_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'white-glass'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'shoes')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'shoes'), os.path.join(dataset_2_path, 'textile_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'shoes'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'clothes')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'clothes'), os.path.join(dataset_2_path, 'textile_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'clothes'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'cardboard')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'cardboard'), os.path.join(dataset_2_path, 'paper_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'cardboard'))\n",
    "\n",
    "dataset_2_electronic_waste_path = os.path.join(dataset_2_path, 'electronic_waste')\n",
    "dataset_2_general_waste_path = os.path.join(dataset_2_path, 'general_waste')\n",
    "dataset_2_glass_waste_path = os.path.join(dataset_2_path, 'glass_waste')\n",
    "dataset_2_metal_waste_path = os.path.join(dataset_2_path, 'metal_waste')\n",
    "dataset_2_paper_waste_path = os.path.join(dataset_2_path, 'paper_waste')\n",
    "dataset_2_plastic_waste_path = os.path.join(dataset_2_path, 'plastic_waste')\n",
    "dataset_2_textile_waste_path = os.path.join(dataset_2_path, 'textile_waste', 'refined_textile_waste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Dataset modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_path = os.path.join(dataset_parent_folder_path, \"Final_Dataset\")\n",
    "final_dataset_general_waste_path = os.path.join(final_dataset_path, \"general_waste\")\n",
    "final_dataset_paper_waste_path = os.path.join(final_dataset_path, \"paper_waste\")\n",
    "final_dataset_plastic_waste_path = os.path.join(final_dataset_path, \"plastic_waste\")\n",
    "final_dataset_metal_waste_path = os.path.join(final_dataset_path, \"metal_waste\")\n",
    "final_dataset_unselected_textile_waste_path = os.path.join(final_dataset_path, \"textile_waste\", 'refined_textile_waste')\n",
    "final_dataset_textile_waste_path = os.path.join(final_dataset_unselected_textile_waste_path, 'selected_refined_textile_waste')\n",
    "final_dataset_electronic_waste_path = os.path.join(final_dataset_path, \"electronic_waste\")\n",
    "final_dataset_glass_waste_path = os.path.join(final_dataset_path, \"glass_waste\")\n",
    "os.makedirs(final_dataset_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_general_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_paper_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_plastic_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_metal_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_unselected_textile_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_textile_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_electronic_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_glass_waste_path, exist_ok=True)\n",
    "\n",
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# clothes_images = [f for f in os.listdir(final_dataset_unselected_textile_waste_path) if f.lower().startswith(\"clothes\")]\n",
    "# shoes_images = [f for f in os.listdir(final_dataset_unselected_textile_waste_path) if f.lower().startswith(\"shoes\")]\n",
    "\n",
    "# random.shuffle(clothes_images)\n",
    "# random.shuffle(shoes_images)\n",
    "\n",
    "# num_images = 1000\n",
    "# selected_clothes_images = clothes_images[:num_images]\n",
    "# selected_shoes_images = shoes_images[:num_images]\n",
    "\n",
    "# for image in selected_clothes_images :\n",
    "#     shutil.copy2(os.path.join(final_dataset_unselected_textile_waste_path, image), os.path.join(final_dataset_textile_waste_path, image))\n",
    "# for image in selected_shoes_images :\n",
    "#     shutil.copy2(os.path.join(final_dataset_unselected_textile_waste_path, image), os.path.join(final_dataset_textile_waste_path, image))\n",
    "\n",
    "# if os.path.exists(final_dataset_electronic_waste_path) and os.path.exists(dataset_2_electronic_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_electronic_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_electronic_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_2_electronic_waste_path, final_dataset_electronic_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_general_waste_path) and os.path.exists(dataset_1_general_waste_path) and os.path.exists(dataset_2_general_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_general_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_general_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_general_waste_path, final_dataset_general_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_general_waste_path, final_dataset_general_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_glass_waste_path) and os.path.exists(dataset_1_glass_waste_path) and os.path.exists(dataset_2_glass_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_glass_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_glass_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_glass_waste_path, final_dataset_glass_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_glass_waste_path, final_dataset_glass_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_metal_waste_path) and os.path.exists(dataset_1_metal_waste_path) and os.path.exists(dataset_2_metal_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_metal_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_metal_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_metal_waste_path, final_dataset_metal_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_metal_waste_path, final_dataset_metal_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_paper_waste_path) and os.path.exists(dataset_1_paper_waste_path) and os.path.exists(dataset_2_paper_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_paper_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_paper_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_paper_waste_path, final_dataset_paper_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_paper_waste_path, final_dataset_paper_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_plastic_waste_path) and os.path.exists(dataset_1_plastic_waste_path) and os.path.exists(dataset_2_plastic_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_plastic_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_plastic_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_plastic_waste_path, final_dataset_plastic_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_plastic_waste_path, final_dataset_plastic_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_textile_waste_path) and os.path.exists(dataset_2_textile_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_textile_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_textile_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_2_textile_waste_path, final_dataset_textile_waste_path, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste types data comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_types = [\"Electronic\", \"General\", \"Paper\", \"Glass\", \"Plastic\", \"Metal\", \"Textile\"]\n",
    "electronic_waste_image_count = count_images_in_folder(final_dataset_electronic_waste_path)\n",
    "general_waste_image_count = count_images_in_folder(final_dataset_general_waste_path)\n",
    "paper_waste_image_count = count_images_in_folder(final_dataset_paper_waste_path)\n",
    "glass_waste_image_count = count_images_in_folder(final_dataset_glass_waste_path)\n",
    "plastic_waste_image_count = count_images_in_folder(final_dataset_plastic_waste_path)\n",
    "metal_waste_image_count = count_images_in_folder(final_dataset_metal_waste_path)\n",
    "textile_waste_image_count = count_images_in_folder(final_dataset_textile_waste_path)\n",
    "\n",
    "image_counts = [electronic_waste_image_count, general_waste_image_count, paper_waste_image_count, glass_waste_image_count, plastic_waste_image_count, metal_waste_image_count, textile_waste_image_count]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "bars = plt.bar(waste_types, image_counts)\n",
    "\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             str(bar.get_height()), ha='center', fontsize=10)\n",
    "    \n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Waste Types\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Histogram of Image Counts by Waste Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data pipeline using tf.keras.utils.image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    final_dataset_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  # For multi-class classification\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224),  # ResNet50 input size\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "class_names = waste_classification_dataset.class_names\n",
    "\n",
    "for images, labels in waste_classification_dataset.take(1):  # Take one batch only\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(9):  # Show the first 9 images\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Convert tensor to image format\n",
    "        plt.title(f\"Label: {class_names[labels[i].numpy().argmax()]}\")  # Display the class index\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# waste_classification_dataset = waste_classification_dataset.shuffle(len(waste_classification_dataset), seed=100, reshuffle_each_iteration=False)\n",
    "# waste_classification_dataset = waste_classification_dataset.shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_size = int(0.7*len(waste_classification_dataset))\n",
    "validation_dataset_size = int(0.15*len(waste_classification_dataset))\n",
    "testing_dataset_size = len(waste_classification_dataset) - training_dataset_size - validation_dataset_size\n",
    "\n",
    "training_dataset = waste_classification_dataset.take(training_dataset_size)\n",
    "validation_dataset = waste_classification_dataset.skip(training_dataset_size).take(validation_dataset_size)\n",
    "testing_dataset = waste_classification_dataset.skip(training_dataset_size+validation_dataset_size).take(testing_dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the first 9 images from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in training_dataset.take(1):  # Take one batch only\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(9):  # Show the first 9 images\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Convert tensor to image format\n",
    "        plt.title(f\"Label: {class_names[labels[i].numpy().argmax()]}\")  # Display the class index\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the first 9 images from the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in validation_dataset.take(1):  # Take one batch only\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(9):  # Show the first 9 images\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Convert tensor to image format\n",
    "        plt.title(f\"Label: {class_names[labels[i].numpy().argmax()]}\")  # Display the class index\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the first 9 images from the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in testing_dataset.take(1):  # Take one batch only\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(9):  # Show the first 9 images\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Convert tensor to image format\n",
    "        plt.title(f\"Label: {class_names[labels[i].numpy().argmax()]}\")  # Display the class index\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the datasets for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.cache()\n",
    "validation_dataset = validation_dataset.cache()\n",
    "testing_dataset = testing_dataset.cache()\n",
    "training_dataset = training_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "testing_dataset = testing_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the datasets for ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.map(lambda x, y: (tf.keras.applications.resnet50.preprocess_input(x), y))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (tf.keras.applications.resnet50.preprocess_input(x), y))\n",
    "testing_dataset = testing_dataset.map(lambda x, y: (tf.keras.applications.resnet50.preprocess_input(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of batches\n",
    "print(len(training_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(len(testing_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ResNet base model for feature extraction ignoring the \"top\" layers used for classification of the ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base model layers\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add a new classification head for our waste classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_resnet50_model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')  # 7 classes\n",
    "])\n",
    "waste_classification_resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_resnet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_resnet50_model_history = waste_classification_resnet50_model.fit(training_dataset, epochs=10, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the ResNet50 model on the testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = waste_classification_resnet50_model.evaluate(testing_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in testing_dataset.take(1):\n",
    "    predictions = waste_classification_resnet50_model.predict(images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(labels.numpy(), axis=1)\n",
    "\n",
    "    # Visualize some images with their predicted labels\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"Pred: {class_names[predicted_classes[i]]}\\nTrue: {class_names[true_classes[i]]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset.unbatch()\n",
    "y_true, y_pred = [], []\n",
    "for images, labels in testing_dataset:\n",
    "    predictions = waste_classification_resnet50_model.predict(tf.expand_dims(images, axis=0))\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    true_class = np.argmax(labels.numpy())\n",
    "    y_pred.append(predicted_class)\n",
    "    y_true.append(true_class)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "plt.savefig(\"resnet50_confusion_matrix.png\", dpi=300)\n",
    "\n",
    "# Classification report\n",
    "classification_report_str = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(\"Classification Report:\\n\", classification_report_str)\n",
    "with open(\"resnet50_waste_classification_report.txt\", \"w\") as file:\n",
    "    file.write(classification_report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_resnet50_model.save('waste_classification_model_with_resnet50_as_basemodel.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune also the top layers of the ResNet50 base model for our waste classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "print('Number of layers in the base model : ', len(base_model.layers))\n",
    "fine_tune_at = 125 # Just an example. Adjust as needed. ResNet50 base model has 175 layers\n",
    "for layer in base_model.layers[:fine_tune_at] :\n",
    "    layer.trainable = False\n",
    "waste_classification_resnet50_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "waste_classification_resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 5\n",
    "initial_epochs = 10\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "fine_tuned_waste_classification_resnet50_model_history = waste_classification_resnet50_model.fit(training_dataset, validation_data=validation_dataset, epochs=total_epochs, initial_epoch=len(waste_classification_resnet50_model_history.epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the finetuned ResNet50 model on the testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_test_loss, finetuned_test_accuracy = waste_classification_resnet50_model.evaluate(testing_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in testing_dataset.take(1):\n",
    "    predictions = waste_classification_resnet50_model.predict(images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(labels.numpy(), axis=1)\n",
    "\n",
    "    # Visualize some images with their predicted labels\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"Pred: {class_names[predicted_classes[i]]}\\nTrue: {class_names[true_classes[i]]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_dataset = testing_dataset.unbatch()\n",
    "y_true, y_pred = [], []\n",
    "for images, labels in testing_dataset:\n",
    "    predictions = waste_classification_resnet50_model.predict(tf.expand_dims(images, axis=0))\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    true_class = np.argmax(labels.numpy())\n",
    "    y_pred.append(predicted_class)\n",
    "    y_true.append(true_class)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"finetuned_resnet50_confusion_matrix.png\", dpi=300)\n",
    "\n",
    "# Classification report\n",
    "classification_report_str_finetuned_model = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(\"Classification Report:\\n\", classification_report_str_finetuned_model)\n",
    "with open(\"finetuned_resnet50_waste_classification_report.txt\", \"w\") as file:\n",
    "    file.write(classification_report_str_finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_resnet50_model.save('waste_classification_finetuned_model_with_resnet50_as_basemodel.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
