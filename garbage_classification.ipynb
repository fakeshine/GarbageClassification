{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.signal as signalpy\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import keras_tuner as kt\n",
    "import kagglehub\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_folder(folder_path, image_extensions=(\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "    count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(image_extensions):  # Check for valid image extensions\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_jpg_images_in_folder(folder_path, image_extensions=(\".jpg\")):\n",
    "    count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(image_extensions):  # Check for valid image extensions\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def load_image_and_label(image_path , label) :\n",
    "    image_string = tf.io.read_file(image_path)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image_decoded.set_shape([224, 224, 3])   \n",
    "    return image_decoded, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the datasets and move them to the repo folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# kaggle_default_dataset_1_path = kagglehub.dataset_download(\"farzadnekouei/trash-type-image-dataset\")\n",
    "# print(\"Path to dataset 1 files :\", kaggle_default_dataset_1_path)\n",
    "# kaggle_default_dataset_2_path = kagglehub.dataset_download(\"mostafaabla/garbage-classification\")\n",
    "# print(\"Path to dataset 2 files :\", kaggle_default_dataset_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# kaggle_default_dataset_1_waste_categories_path = os.path.join(kaggle_default_dataset_1_path, 'TrashType_Image_Dataset')\n",
    "# kaggle_default_dataset_2_waste_categories_path = os.path.join(kaggle_default_dataset_2_path, 'garbage_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_folder_path = os.path.join(os.getcwd(),'Dataset')\n",
    "dataset_1_path = os.path.join(dataset_parent_folder_path, 'Dataset 1')\n",
    "dataset_2_path = os.path.join(dataset_parent_folder_path, 'Dataset 2')\n",
    "os.makedirs(dataset_parent_folder_path, exist_ok=True)\n",
    "os.makedirs(dataset_1_path, exist_ok=True)\n",
    "os.makedirs(dataset_2_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# shutil.copytree(kaggle_default_dataset_1_waste_categories_path, dataset_1_path, dirs_exist_ok=True)\n",
    "# shutil.rmtree(kaggle_default_dataset_1_path)\n",
    "# shutil.copytree(kaggle_default_dataset_2_waste_categories_path, dataset_2_path, dirs_exist_ok=True)\n",
    "# shutil.rmtree(kaggle_default_dataset_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give a generic structure to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waste Types Folders :\n",
    "1. general_waste\n",
    "2. paper_waste\n",
    "3. plastic_waste\n",
    "4. metal_waste\n",
    "5. textile_waste\n",
    "6. electronic_waste\n",
    "7. glass_waste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 1 modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'glass')) and not os.path.exists(os.path.join(dataset_1_path, 'glass_waste')) :\n",
    "#     os.replace(os.path.join(dataset_1_path, 'glass'), os.path.join(dataset_1_path, 'glass_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'metal')) and not os.path.exists(os.path.join(dataset_1_path, 'metal_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'metal'), os.path.join(dataset_1_path, 'metal_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'paper')) and not os.path.exists(os.path.join(dataset_1_path, 'paper_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'paper'), os.path.join(dataset_1_path, 'paper_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'plastic')) and not os.path.exists(os.path.join(dataset_1_path, 'plastic_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'plastic'), os.path.join(dataset_1_path, 'plastic_waste'))\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'trash')) and not os.path.exists(os.path.join(dataset_1_path, 'general_waste')) :\n",
    "#     os.rename(os.path.join(dataset_1_path, 'trash'), os.path.join(dataset_1_path, 'general_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_1_path, 'cardboard')) :\n",
    "#     shutil.copytree(os.path.join(dataset_1_path, 'cardboard'), os.path.join(dataset_1_path, 'paper_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_1_path, 'cardboard'))\n",
    "\n",
    "dataset_1_general_waste_path = os.path.join(dataset_1_path, 'general_waste')\n",
    "dataset_1_glass_waste_path = os.path.join(dataset_1_path, 'glass_waste')\n",
    "dataset_1_metal_waste_path = os.path.join(dataset_1_path, 'metal_waste')\n",
    "dataset_1_paper_waste_path = os.path.join(dataset_1_path, 'paper_waste')\n",
    "dataset_1_plastic_waste_path = os.path.join(dataset_1_path, 'plastic_waste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2 modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'battery')) and not os.path.exists(os.path.join(dataset_2_path, 'electronic_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'battery'), os.path.join(dataset_2_path, 'electronic_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'trash')) and not os.path.exists(os.path.join(dataset_2_path, 'general_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'trash'), os.path.join(dataset_2_path, 'general_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'metal')) and not os.path.exists(os.path.join(dataset_2_path, 'metal_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'metal'), os.path.join(dataset_2_path, 'metal_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'paper')) and not os.path.exists(os.path.join(dataset_2_path, 'paper_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'paper'), os.path.join(dataset_2_path, 'paper_waste'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'plastic')) and not os.path.exists(os.path.join(dataset_2_path, 'plastic_waste')) :\n",
    "#     os.replace(os.path.join(dataset_2_path, 'plastic'), os.path.join(dataset_2_path, 'plastic_waste'))\n",
    "\n",
    "# os.makedirs(os.path.join(dataset_2_path, 'glass_waste'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(dataset_2_path, 'textile_waste'), exist_ok=True)\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'biological')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'biological'), os.path.join(dataset_2_path, 'general_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'biological'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'brown-glass')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'brown-glass'), os.path.join(dataset_2_path, 'glass_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'brown-glass'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'green-glass')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'green-glass'), os.path.join(dataset_2_path, 'glass_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'green-glass'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'white-glass')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'white-glass'), os.path.join(dataset_2_path, 'glass_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'white-glass'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'shoes')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'shoes'), os.path.join(dataset_2_path, 'textile_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'shoes'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'clothes')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'clothes'), os.path.join(dataset_2_path, 'textile_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'clothes'))\n",
    "\n",
    "# if os.path.exists(os.path.join(dataset_2_path, 'cardboard')) :\n",
    "#     shutil.copytree(os.path.join(dataset_2_path, 'cardboard'), os.path.join(dataset_2_path, 'paper_waste'), dirs_exist_ok=True)\n",
    "#     shutil.rmtree(os.path.join(dataset_2_path, 'cardboard'))\n",
    "\n",
    "dataset_2_electronic_waste_path = os.path.join(dataset_2_path, 'electronic_waste')\n",
    "dataset_2_general_waste_path = os.path.join(dataset_2_path, 'general_waste')\n",
    "dataset_2_glass_waste_path = os.path.join(dataset_2_path, 'glass_waste')\n",
    "dataset_2_metal_waste_path = os.path.join(dataset_2_path, 'metal_waste')\n",
    "dataset_2_paper_waste_path = os.path.join(dataset_2_path, 'paper_waste')\n",
    "dataset_2_plastic_waste_path = os.path.join(dataset_2_path, 'plastic_waste')\n",
    "dataset_2_textile_waste_path = os.path.join(dataset_2_path, 'textile_waste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Dataset modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_path = os.path.join(dataset_parent_folder_path, \"Final_Dataset\")\n",
    "final_dataset_general_waste_path = os.path.join(final_dataset_path, \"general_waste\")\n",
    "final_dataset_paper_waste_path = os.path.join(final_dataset_path, \"paper_waste\")\n",
    "final_dataset_plastic_waste_path = os.path.join(final_dataset_path, \"plastic_waste\")\n",
    "final_dataset_metal_waste_path = os.path.join(final_dataset_path, \"metal_waste\")\n",
    "final_dataset_textile_waste_path = os.path.join(final_dataset_path, \"textile_waste\")\n",
    "final_dataset_electronic_waste_path = os.path.join(final_dataset_path, \"electronic_waste\")\n",
    "final_dataset_glass_waste_path = os.path.join(final_dataset_path, \"glass_waste\")\n",
    "os.makedirs(final_dataset_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_general_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_paper_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_plastic_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_metal_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_textile_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_electronic_waste_path, exist_ok=True)\n",
    "os.makedirs(final_dataset_glass_waste_path, exist_ok=True)\n",
    "\n",
    "# UNCOMMENT WHEN SETTING THE DATASET UP FOR THE FIRST TIME\n",
    "\n",
    "# if os.path.exists(final_dataset_electronic_waste_path) and os.path.exists(dataset_2_electronic_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_electronic_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_electronic_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_2_electronic_waste_path, final_dataset_electronic_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_general_waste_path) and os.path.exists(dataset_1_general_waste_path) and os.path.exists(dataset_2_general_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_general_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_general_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_general_waste_path, final_dataset_general_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_general_waste_path, final_dataset_general_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_glass_waste_path) and os.path.exists(dataset_1_glass_waste_path) and os.path.exists(dataset_2_glass_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_glass_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_glass_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_glass_waste_path, final_dataset_glass_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_glass_waste_path, final_dataset_glass_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_metal_waste_path) and os.path.exists(dataset_1_metal_waste_path) and os.path.exists(dataset_2_metal_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_metal_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_metal_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_metal_waste_path, final_dataset_metal_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_metal_waste_path, final_dataset_metal_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_paper_waste_path) and os.path.exists(dataset_1_paper_waste_path) and os.path.exists(dataset_2_paper_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_paper_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_paper_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_paper_waste_path, final_dataset_paper_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_paper_waste_path, final_dataset_paper_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_plastic_waste_path) and os.path.exists(dataset_1_plastic_waste_path) and os.path.exists(dataset_2_plastic_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_plastic_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_plastic_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_1_plastic_waste_path, final_dataset_plastic_waste_path, dirs_exist_ok=True)\n",
    "#     shutil.copytree(dataset_2_plastic_waste_path, final_dataset_plastic_waste_path, dirs_exist_ok=True)\n",
    "\n",
    "# if os.path.exists(final_dataset_textile_waste_path) and os.path.exists(dataset_2_textile_waste_path) :\n",
    "#     for item in os.listdir(final_dataset_textile_waste_path) :\n",
    "#         item_path = os.path.join(final_dataset_textile_waste_path, item)\n",
    "#         if os.path.isfile(item_path) or os.path.islink(item_path) :\n",
    "#             os.unlink(item_path)\n",
    "#         elif os.path.isdir(item_path) :\n",
    "#             shutil.rmtree(item_path)\n",
    "#     shutil.copytree(dataset_2_textile_waste_path, final_dataset_textile_waste_path, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste types data comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_types = [\"Electronic\", \"General\", \"Paper\", \"Glass\", \"Plastic\", \"Metal\", \"Textile\"]\n",
    "electronic_waste_image_count = count_images_in_folder(final_dataset_electronic_waste_path)\n",
    "general_waste_image_count = count_images_in_folder(final_dataset_general_waste_path)\n",
    "paper_waste_image_count = count_images_in_folder(final_dataset_paper_waste_path)\n",
    "glass_waste_image_count = count_images_in_folder(final_dataset_glass_waste_path)\n",
    "plastic_waste_image_count = count_images_in_folder(final_dataset_plastic_waste_path)\n",
    "metal_waste_image_count = count_images_in_folder(final_dataset_metal_waste_path)\n",
    "textile_waste_image_count = count_images_in_folder(final_dataset_textile_waste_path)\n",
    "\n",
    "image_counts = [electronic_waste_image_count, general_waste_image_count, paper_waste_image_count, glass_waste_image_count, plastic_waste_image_count, metal_waste_image_count, textile_waste_image_count]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "bars = plt.bar(waste_types, image_counts)\n",
    "\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             str(bar.get_height()), ha='center', fontsize=10)\n",
    "    \n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Waste Types\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Histogram of Image Counts by Waste Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data pipeline. 1st way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    final_dataset_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  # For multi-class classification\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224),  # ResNet50 input size\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "waste_classification_dataset = waste_classification_dataset.map(lambda x, y: (tf.keras.applications.resnet50.preprocess_input(x), y))\n",
    "waste_classification_dataset = waste_classification_dataset.shuffle(len(waste_classification_dataset), seed=100, reshuffle_each_iteration=False)\n",
    "# waste_classification_dataset = waste_classification_dataset.shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_size = int(0.7*len(waste_classification_dataset))\n",
    "validation_dataset_size = int(0.15*len(waste_classification_dataset))\n",
    "testing_dataset_size = len(waste_classification_dataset) - training_dataset_size - validation_dataset_size\n",
    "\n",
    "training_dataset = waste_classification_dataset.take(training_dataset_size)\n",
    "validation_dataset = waste_classification_dataset.skip(training_dataset_size).take(validation_dataset_size)\n",
    "testing_dataset = waste_classification_dataset.skip(training_dataset_size+validation_dataset_size).take(testing_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of batches\n",
    "print(len(training_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(len(testing_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_array = []\n",
    "# for sample, label in training_dataset.as_numpy_iterator() :\n",
    "#     labels_array.append(label)\n",
    "# print(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.cache()\n",
    "validation_dataset = validation_dataset.cache()\n",
    "testing_dataset = testing_dataset.cache()\n",
    "training_dataset = training_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "testing_dataset = testing_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "# Add a new classifier head\n",
    "waste_classification_resnet50_model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')  # 7 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "waste_classification_resnet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the prepared dataset\n",
    "waste_classification_resnet50_model.fit(training_dataset, epochs=10, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "waste_classification_resnet50_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "waste_classification_resnet50_model.fit(training_dataset, validation_data=validation_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_resnet50_model.evaluate(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_resnet50_model.save('waste_classification_resnet50_finetuned_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data pipeline. 2nd way. Didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electronic_waste_tf_dataset_without_labels = tf.data.Dataset.list_files(final_dataset_electronic_waste_path + '/*.jpg')\n",
    "general_waste_tf_dataset_without_labels = tf.data.Dataset.list_files(final_dataset_general_waste_path + '/*.jpg')\n",
    "glass_waste_tf_dataset_without_labels = tf.data.Dataset.list_files(final_dataset_glass_waste_path + '/*.jpg')\n",
    "metal_waste_tf_dataset_without_labels = tf.data.Dataset.list_files(final_dataset_metal_waste_path + '/*.jpg')\n",
    "paper_waste_tf_dataset_without_labels = tf.data.Dataset.list_files(final_dataset_paper_waste_path + '/*.jpg')\n",
    "plastic_waste_tf_dataset_without_labels = tf.data.Dataset.list_files(final_dataset_plastic_waste_path + '/*.jpg')\n",
    "textile_waste_tf_dataset_without_labels = tf.data.Dataset.list_files(final_dataset_textile_waste_path + '/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(electronic_waste_tf_dataset_without_labels.as_numpy_iterator().next())\n",
    "print(general_waste_tf_dataset_without_labels.as_numpy_iterator().next())\n",
    "print(glass_waste_tf_dataset_without_labels.as_numpy_iterator().next())\n",
    "print(metal_waste_tf_dataset_without_labels.as_numpy_iterator().next())\n",
    "print(paper_waste_tf_dataset_without_labels.as_numpy_iterator().next())\n",
    "print(plastic_waste_tf_dataset_without_labels.as_numpy_iterator().next())\n",
    "print(textile_waste_tf_dataset_without_labels.as_numpy_iterator().next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electronic_waste_tf_dataset_with_labels = tf.data.Dataset.zip((electronic_waste_tf_dataset_without_labels, tf.data.Dataset.from_tensor_slices(tf.zeros(len(electronic_waste_tf_dataset_without_labels), dtype=tf.int32))))\n",
    "general_waste_tf_dataset_with_labels = tf.data.Dataset.zip((general_waste_tf_dataset_without_labels, tf.data.Dataset.from_tensor_slices(tf.ones(len(general_waste_tf_dataset_without_labels), dtype=tf.int32))))\n",
    "glass_waste_tf_dataset_with_labels = tf.data.Dataset.zip(glass_waste_tf_dataset_without_labels, tf.data.Dataset.from_tensor_slices(tf.fill([len(glass_waste_tf_dataset_without_labels)], 2)))\n",
    "metal_waste_tf_dataset_with_labels = tf.data.Dataset.zip(metal_waste_tf_dataset_without_labels, tf.data.Dataset.from_tensor_slices(tf.fill([len(metal_waste_tf_dataset_without_labels)], 3)))\n",
    "paper_waste_tf_dataset_with_labels = tf.data.Dataset.zip(paper_waste_tf_dataset_without_labels, tf.data.Dataset.from_tensor_slices(tf.fill([len(paper_waste_tf_dataset_without_labels)], 4)))\n",
    "plastic_waste_tf_dataset_with_labels = tf.data.Dataset.zip(plastic_waste_tf_dataset_without_labels, tf.data.Dataset.from_tensor_slices(tf.fill([len(plastic_waste_tf_dataset_without_labels)], 5)))\n",
    "textile_waste_tf_dataset_with_labels = tf.data.Dataset.zip(textile_waste_tf_dataset_without_labels, tf.data.Dataset.from_tensor_slices(tf.fill([len(textile_waste_tf_dataset_without_labels)], 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(electronic_waste_tf_dataset_with_labels.as_numpy_iterator().next())\n",
    "print(general_waste_tf_dataset_with_labels.as_numpy_iterator().next())\n",
    "print(glass_waste_tf_dataset_with_labels.as_numpy_iterator().next())\n",
    "print(metal_waste_tf_dataset_with_labels.as_numpy_iterator().next())\n",
    "print(paper_waste_tf_dataset_with_labels.as_numpy_iterator().next())\n",
    "print(plastic_waste_tf_dataset_with_labels.as_numpy_iterator().next())\n",
    "print(textile_waste_tf_dataset_with_labels.as_numpy_iterator().next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_dataset = electronic_waste_tf_dataset_with_labels.concatenate(general_waste_tf_dataset_with_labels).concatenate(glass_waste_tf_dataset_with_labels).concatenate(metal_waste_tf_dataset_with_labels).concatenate(paper_waste_tf_dataset_with_labels).concatenate(plastic_waste_tf_dataset_with_labels).concatenate(textile_waste_tf_dataset_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(waste_classification_dataset.as_numpy_iterator().next())\n",
    "len(waste_classification_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffling the dataset. Otherwise, the training/testing data will be unbalanced (the labels are in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_classification_dataset = waste_classification_dataset.shuffle(len(waste_classification_dataset), seed=100, reshuffle_each_iteration=False)\n",
    "labels_array = []\n",
    "for sample, label in waste_classification_dataset.as_numpy_iterator() :\n",
    "    labels_array.append(label)\n",
    "\n",
    "print(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = waste_classification_dataset.take(len(waste_classification_dataset)*7//10)\n",
    "validation_data = waste_classification_dataset.skip(len(training_data)).take(len(waste_classification_dataset)*15//100)\n",
    "testing_data = waste_classification_dataset.skip(len(training_data)+len(validation_data)).take(len(waste_classification_dataset)-len(training_data)-len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR TREBUI SA FOLOSIM tf.keras.applications.resnet.preprocess_input ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))\n",
    "print(len(validation_data))\n",
    "print(len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = []\n",
    "for sample, label in training_data.as_numpy_iterator() :\n",
    "    labels_array.append(label)\n",
    "print(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = []\n",
    "for sample, label in validation_data.as_numpy_iterator() :\n",
    "    labels_array.append(label)\n",
    "print(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = []\n",
    "for sample, label in testing_data.as_numpy_iterator() :\n",
    "    labels_array.append(label)\n",
    "print(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.map(load_image_and_label)\n",
    "training_data = training_data.cache()\n",
    "training_data = training_data.batch(32)\n",
    "training_data = training_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = testing_data.map(load_image_and_label)\n",
    "testing_data = testing_data.cache()\n",
    "testing_data = testing_data.batch(32)\n",
    "testing_data = testing_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = validation_data.map(load_image_and_label)\n",
    "validation_data = validation_data.cache()\n",
    "validation_data = validation_data.batch(32)\n",
    "validation_data = validation_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing one batch of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples , labels = training_data.as_numpy_iterator().next()\n",
    "print(samples.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples , labels = testing_data.as_numpy_iterator().next()\n",
    "print(samples.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples , labels = validation_data.as_numpy_iterator().next()\n",
    "print(samples.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_shape_array = []\n",
    "for samples, labels in training_data.as_numpy_iterator() :\n",
    "    samples_shape_array.append(str(samples.shape))\n",
    "samples_shape_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
